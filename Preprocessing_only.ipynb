{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\peleg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_POS = \"../../text/twitter-datasets/train_pos.txt\"\n",
    "PATH_TO_NEG = \"../../text/twitter-datasets/train_neg.txt\"\n",
    "proportion_train = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(ys, preds):\n",
    "    matrix = [[0, 0], [0, 0]]\n",
    "    for y,pred in zip(ys, preds):\n",
    "        if pred == y:\n",
    "            if y == 1:\n",
    "                matrix[0][0] += 1\n",
    "            else:\n",
    "                matrix[1][1] += 1\n",
    "        else:\n",
    "            if y == 1:\n",
    "                matrix[1][0] += 1\n",
    "            else:\n",
    "                matrix[0][1] += 1\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(matrix):\n",
    "    corr_guesses = matrix[0][0] + matrix[1][1]\n",
    "    total_guesses = matrix[0][0] + matrix[0][1] + matrix[1][0] + matrix[1][1]\n",
    "    return corr_guesses / total_guesses\n",
    "def compute_precision(matrix):\n",
    "    true_pos = matrix[0][0]\n",
    "    false_pos = matrix[0][1]\n",
    "    return true_pos / (true_pos+false_pos)\n",
    "def compute_recall(matrix):\n",
    "    true_pos = matrix[0][0]\n",
    "    false_neg = matrix[1][0]\n",
    "    return true_pos / (true_pos+false_neg)\n",
    "def compute_fscore(matrix):\n",
    "    p = compute_precision(matrix)\n",
    "    r = compute_recall(matrix)\n",
    "    return 2 * (p*r) / (p+r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data retrieving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_file(filename, proportion=None, isTraining=False, value=None, shuffle=False):\n",
    "    \"\"\"We assume here that (proportion != None) <=> local testing. \n",
    "    Please be sure to verify this before using the function.\n",
    "    The shuffle part is only used for the local testing phase, \n",
    "    where selecting different subsets to train/validate our model can be of influence\"\"\"\n",
    "    with open(filename, \"r\") as file:\n",
    "        content = file.read()\n",
    "        content_lines = content.split(\"\\n\")\n",
    "        if shuffle:\n",
    "            random.shuffle(content_lines)\n",
    "        if proportion != None:\n",
    "            # Here is the processing of training data during the local testing phase\n",
    "            temp1_x = content_lines[:int(len(content_lines) * proportion)]\n",
    "            temp1_y = [value] * int(len(content_lines) * proportion)\n",
    "            temp2_x = content_lines[int(len(content_lines) * proportion):]\n",
    "            temp2_y = [value] * int(len(content_lines) * (1-proportion))\n",
    "            return temp1_x, temp1_y, temp2_x, temp2_y\n",
    "        if isTraining:\n",
    "            # Here is the processing of training data during the real prediction phase\n",
    "            temp_x = content_lines[:]\n",
    "            temp_y = [value] * len(content_lines)\n",
    "            return temp_x, temp_y\n",
    "        # Here is the processing of new data for the real prediction phase\n",
    "        temp_ids = []\n",
    "        temp_xs = []\n",
    "        for i in range(len(content_lines)):\n",
    "            if \",\" in content_lines[i]:\n",
    "                entrySplitted = re.split(\",\", content_lines[i], 1)\n",
    "                temp_ids.append(entrySplitted[0])\n",
    "                temp_xs.append(entrySplitted[1])\n",
    "        return temp_ids, temp_xs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_words = words.words()\n",
    "stop_words = stopwords.words()\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_sentence(val, depth=0):\n",
    "#     \"\"\"1. Remove non alphanumerical characters\n",
    "#        2. Remove stop words\n",
    "#        3. Error correction\n",
    "#        4. Stemming\"\"\"\n",
    "#     sentence = val\n",
    "#     if depth > 0:\n",
    "#         regex = re.compile('([^\\s\\w]|_)+')\n",
    "#         sentence = regex.sub('', val).lower()\n",
    "#         if depth > 1:\n",
    "#             sentence = word_tokenize(sentence)\n",
    "#             for word in list(sentence):\n",
    "#                 if word in stop_words:\n",
    "#                     sentence.remove(word)\n",
    "#                 elif depth > 2:\n",
    "#                     # words_list = [\n",
    "#                     #     (jaccard_distance(set(ngrams(word, 2)), set(ngrams(w, 2))),w)\n",
    "#                     #     for w in correct_words if w[0]==word[0]\n",
    "#                     # ]\n",
    "#                     try:\n",
    "#                         word_best = sorted(words_list, key = lambda val:val[0])[0][1]\n",
    "#                     except:\n",
    "#                         word_best = word\n",
    "#                     if depth > 3 :\n",
    "#                         sentence[sentence.index(word)] = ps.stem(word_best)\n",
    "#                     else:\n",
    "#                         sentence[sentence.index(word)] = word_best\n",
    "#             sentence = \" \".join(sentence)\n",
    "#     return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(val, steps=[0]):\n",
    "    \"\"\"\n",
    "        0. Nothing\n",
    "        1. Remove non alphanumerical characters\n",
    "        2. Remove stop words\n",
    "        3. Stemming\n",
    "    \"\"\"\n",
    "    sentence = val\n",
    "    if 0 in steps:\n",
    "        return sentence\n",
    "    if 1 in steps:\n",
    "        regex = re.compile('([^\\s\\w]|_)+')\n",
    "        sentence = regex.sub('', val).lower()\n",
    "    sentence = word_tokenize(sentence)\n",
    "    if 2 in steps:\n",
    "        for word in sentence:\n",
    "            if word in stop_words:\n",
    "                sentence.remove(word)\n",
    "            elif 3 in steps:\n",
    "                sentence[sentence.index(word)] = ps.stem(word)\n",
    "    elif 3 in steps:\n",
    "        for word in sentence:\n",
    "            sentence[sentence.index(word)] = ps.stem(word)\n",
    "    sentence = \" \".join(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data, depth=0):\n",
    "    for i in tqdm(range(len(data))):\n",
    "        data[i] = clean_sentence(data[i], depth)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_ngrams(words, n):\n",
    "    ngrams = []\n",
    "    number_ngrams = len(words)-n+1\n",
    "    for i in range(number_ngrams):\n",
    "        ngram = \"\"\n",
    "        for j in range(n):\n",
    "            ngram += words[i+j]\n",
    "            if j != n-1:\n",
    "                ngram += \" \"\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_ngrams_table_constructor(xs, ys, n=2):\n",
    "    table = {}\n",
    "    for x,y in zip(xs, ys):\n",
    "        list_words = x.split(\" \")\n",
    "        list_ngrams = form_ngrams(list_words, n=n)\n",
    "        for ngram in list_ngrams:\n",
    "            if ngram not in table:\n",
    "                table[ngram] = [3, 1, 1]\n",
    "                if y == 1:\n",
    "                    table[ngram][1] += 1\n",
    "                else:\n",
    "                    table[ngram][2] += 1\n",
    "            else:\n",
    "                table[ngram][0] += 1\n",
    "                if y == 1:\n",
    "                    table[ngram][1] += 1\n",
    "                else:\n",
    "                    table[ngram][2] += 1\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proba_ngram(sentence, table, n):\n",
    "    \"\"\"Please make sure that the table have been generated with the same n.\"\"\"\n",
    "    proba_pos = 1\n",
    "    proba_neg = 1\n",
    "    list_words = sentence.split(\" \")\n",
    "    list_ngrams = form_ngrams(list_words, n=n)\n",
    "    for ngram in list_ngrams:\n",
    "        if ngram in table:\n",
    "            proba_pos *= table[ngram][1]/table[ngram][0]\n",
    "            proba_neg *= table[ngram][2]/table[ngram][0]\n",
    "        else:\n",
    "            proba_pos *= 0.5\n",
    "            proba_neg *= 0.5\n",
    "    return proba_pos, proba_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_sentences(sentences, ns, tables, classes=None):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    if classes == None:\n",
    "        for sentence in sentences:\n",
    "            probas = []\n",
    "            for n in ns:\n",
    "                probas += get_proba_ngram(sentence, tables[n], n=n)\n",
    "            xs.append(probas)\n",
    "        return np.array(xs)\n",
    "    for sentence,y in zip(sentences, classes):\n",
    "        probas = []\n",
    "        for n in ns:\n",
    "            probas += get_proba_ngram(sentence, tables[n], n=n)\n",
    "        xs.append(probas)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entries formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_x_raw = []\n",
    "train_sentences_y = []\n",
    "test_sentences_x_raw = []\n",
    "test_y = []\n",
    "\n",
    "temp_train_x, temp_train_y, temp_test_x, temp_test_y = get_data_from_file(\n",
    "    PATH_TO_POS, \n",
    "    proportion=proportion_train,\n",
    "    isTraining=True,\n",
    "    value=1,\n",
    "    shuffle=False\n",
    ")\n",
    "train_sentences_x_raw += temp_train_x\n",
    "train_sentences_y += temp_train_y\n",
    "test_sentences_x_raw += temp_test_x\n",
    "test_y += temp_test_y\n",
    "temp_train_x, temp_train_y, temp_test_x, temp_test_y = get_data_from_file(\n",
    "    PATH_TO_NEG, \n",
    "    proportion=proportion_train,\n",
    "    isTraining=True,\n",
    "    value=0,\n",
    "    shuffle=False\n",
    ")\n",
    "train_sentences_x_raw += temp_train_x\n",
    "train_sentences_y += temp_train_y\n",
    "test_sentences_x_raw += temp_test_x\n",
    "test_y += temp_test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,), (1,), (2,), (3,), (1, 2), (1, 3), (2, 3), (1, 2, 3)]\n"
     ]
    }
   ],
   "source": [
    "steps_pool = [0, 1, 2, 3]\n",
    "steps =[]\n",
    "for l in range(0, len(steps_pool)+1):\n",
    "    for subset in itertools.combinations(steps_pool, l):\n",
    "        steps.append(subset)\n",
    "for subset in list(steps):\n",
    "    if (0 in subset and len(subset)>1) or len(subset) == 0:\n",
    "        steps.remove(subset)\n",
    "print(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160000/160000 [00:00<00:00, 2342540.43it/s]\n",
      "100%|██████████| 160000/160000 [00:17<00:00, 9054.52it/s] \n",
      "100%|██████████| 160000/160000 [03:09<00:00, 844.04it/s] \n",
      "100%|██████████| 160000/160000 [00:56<00:00, 2836.27it/s]\n",
      "100%|██████████| 160000/160000 [02:03<00:00, 1296.40it/s]\n",
      "100%|██████████| 160000/160000 [00:47<00:00, 3377.36it/s]\n",
      "100%|██████████| 160000/160000 [03:33<00:00, 747.85it/s]\n",
      "100%|██████████| 160000/160000 [02:26<00:00, 1089.57it/s]\n"
     ]
    }
   ],
   "source": [
    "likelihood_tables = {}\n",
    "\n",
    "for i,curr_steps in enumerate(steps):\n",
    "    likelihood_tables[i] = likelihood_ngrams_table_constructor(\n",
    "        clean_data(train_sentences_x_raw[:], curr_steps),\n",
    "        train_sentences_y,\n",
    "        n=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40002/40002 [00:00<00:00, 1581567.13it/s]\n",
      "100%|██████████| 40002/40002 [00:04<00:00, 9261.92it/s]\n",
      "100%|██████████| 40002/40002 [00:46<00:00, 856.66it/s] \n",
      "100%|██████████| 40002/40002 [00:13<00:00, 2952.20it/s]\n",
      "100%|██████████| 40002/40002 [00:31<00:00, 1279.04it/s]\n",
      "100%|██████████| 40002/40002 [00:11<00:00, 3371.46it/s]\n",
      "100%|██████████| 40002/40002 [00:55<00:00, 724.44it/s]\n",
      "100%|██████████| 40002/40002 [00:36<00:00, 1086.47it/s]\n"
     ]
    }
   ],
   "source": [
    "test_datas_x = {}\n",
    "\n",
    "for i,curr_steps in enumerate(steps):\n",
    "    test_datas_x[i] = get_data_from_sentences(\n",
    "        clean_data(test_sentences_x_raw[:], curr_steps),\n",
    "        ns = [2],\n",
    "        tables = {2: likelihood_tables[i]}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_y = np.array(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_bigram(xs):\n",
    "    preds = []\n",
    "    for x in xs:\n",
    "        probaPos = x[0]\n",
    "        probaNeg = x[1]\n",
    "        if probaPos>probaNeg:\n",
    "            preds.append(1)\n",
    "        else:\n",
    "            preds.append(0)\n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = {}\n",
    "\n",
    "for d in range(len(steps)):\n",
    "    preds[d] = evaluator_bigram(test_datas_x[d])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: No preprocessing\n",
      "1: Non alphanumeric characters removal\n",
      "2: 1 + Stop words removal\n",
      "3: 2 + Stemming\n",
      "Depth (0,)\n",
      "acc=0.799, prec=0.766, rec=0.860, fscore=0.810\n",
      "Depth (1,)\n",
      "acc=0.789, prec=0.763, rec=0.837, fscore=0.798\n",
      "Depth (2,)\n",
      "acc=0.766, prec=0.720, rec=0.869, fscore=0.788\n",
      "Depth (3,)\n",
      "acc=0.785, prec=0.740, rec=0.879, fscore=0.804\n",
      "Depth (1, 2)\n",
      "acc=0.758, prec=0.743, rec=0.791, fscore=0.766\n",
      "Depth (1, 3)\n",
      "acc=0.788, prec=0.764, rec=0.834, fscore=0.798\n",
      "Depth (2, 3)\n",
      "acc=0.765, prec=0.720, rec=0.867, fscore=0.787\n",
      "Depth (1, 2, 3)\n",
      "acc=0.756, prec=0.740, rec=0.790, fscore=0.764\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"0: No preprocessing\\n\"\n",
    "    \"1: Non alphanumeric characters removal\\n\"\n",
    "    \"2: 1 + Stop words removal\\n\"\n",
    "    \"3: 2 + Stemming\"\n",
    ")\n",
    "\n",
    "for i,curr_steps in enumerate(steps):\n",
    "    matrix = get_confusion_matrix(test_data_y, preds[i])\n",
    "    print(\n",
    "        (\"Depth {}\\n\"\n",
    "        # \"{}\\n\"\n",
    "        # \"{}\\n\"\n",
    "        \"acc={:.3f}, prec={:.3f}, rec={:.3f}, fscore={:.3f}\").format(\n",
    "            curr_steps,\n",
    "            # matrix[0],\n",
    "            # matrix[1],\n",
    "            compute_accuracy(matrix),\n",
    "            compute_precision(matrix),\n",
    "            compute_recall(matrix),\n",
    "            compute_fscore(matrix)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c19f1ca3afd4998816691da9a36584d647e992dcfc819c425cd38dca0566a7c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
