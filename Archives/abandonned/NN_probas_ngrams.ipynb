{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_file(filename, proportion=None, isTraining=False, value=None, shuffle=False):\n",
    "    \"\"\"We assume here that (proportion != None) <=> local testing. \n",
    "    Please be sure to verify this before using the function.\n",
    "    The shuffle part is only used for the local testing phase, \n",
    "    where selecting different subsets to train/validate our model can be of influence\"\"\"\n",
    "    with open(filename, \"r\") as file:\n",
    "        content = file.read()\n",
    "        content_lines = content.split(\"\\n\")\n",
    "        for i in range(len(content_lines)):\n",
    "            if content_lines[len(content_lines)-1-i] == \"\" or content_lines[len(content_lines)-1-i] == \" \":\n",
    "                del content_lines[len(content_lines)-1-i]\n",
    "        if shuffle:\n",
    "            random.shuffle(content_lines)\n",
    "        if proportion != None:\n",
    "            # Here is the processing of training data during the local testing phase\n",
    "            temp1_x = content_lines[:int(len(content_lines) * proportion)]\n",
    "            temp1_y = [value] * int(len(content_lines) * proportion)\n",
    "            temp2_x = content_lines[int(len(content_lines) * proportion):]\n",
    "            temp2_y = [value] * (int(len(content_lines) * (1-proportion)) + 1)\n",
    "            return temp1_x, temp1_y, temp2_x, temp2_y\n",
    "        if isTraining:\n",
    "            # Here is the processing of training data during the real prediction phase\n",
    "            temp_x = content_lines[:]\n",
    "            temp_y = [value] * len(content_lines)\n",
    "            return temp_x, temp_y\n",
    "        # Here is the processing of new data for the real prediction phase\n",
    "        temp_ids = []\n",
    "        temp_xs = []\n",
    "        for i in range(len(content_lines)):\n",
    "            if \",\" in content_lines[i]:\n",
    "                entrySplitted = re.split(\",\", content_lines[i], 1)\n",
    "                temp_ids.append(entrySplitted[0])\n",
    "                temp_xs.append(entrySplitted[1])\n",
    "        return temp_ids, temp_xs        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_POS = \"../../text/twitter-datasets/train_pos.txt\"\n",
    "PATH_TO_NEG = \"../../text/twitter-datasets/train_neg.txt\"\n",
    "PATH_TO_TEST = \"../../text/twitter-datasets/test_data.txt\"\n",
    "PATH_TO_SUB = \"./submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_train = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 ?= 19999\n",
      "40000 ?= 39998\n"
     ]
    }
   ],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "test_x = []\n",
    "test_y = []\n",
    "\n",
    "temp_train_x, temp_train_y, temp_test_x, temp_test_y = get_data_from_file(\n",
    "    PATH_TO_POS, \n",
    "    proportion=proportion_train,\n",
    "    isTraining=True,\n",
    "    value=1,\n",
    "    shuffle=False\n",
    ")\n",
    "train_x += temp_train_x\n",
    "train_y += temp_train_y\n",
    "test_x += temp_test_x\n",
    "test_y += temp_test_y\n",
    "print(\"{} ?= {}\".format(\n",
    "    len(test_x),\n",
    "    len(test_y)\n",
    "))\n",
    "temp_train_x, temp_train_y, temp_test_x, temp_test_y = get_data_from_file(\n",
    "    PATH_TO_NEG, \n",
    "    proportion=proportion_train,\n",
    "    isTraining=True,\n",
    "    value=0,\n",
    "    shuffle=False\n",
    ")\n",
    "train_x += temp_train_x\n",
    "train_y += temp_train_y\n",
    "test_x += temp_test_x\n",
    "test_y += temp_test_y\n",
    "print(\"{} ?= {}\".format(\n",
    "    len(test_x),\n",
    "    len(test_y)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting the inputs dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = [1, 2, 3]          # We use n-grams with 1<=n<=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_ngrams(words, n):\n",
    "    ngrams = []\n",
    "    number_ngrams = len(words)-n+1\n",
    "    for i in range(number_ngrams):\n",
    "        ngram = \"\"\n",
    "        for j in range(n):\n",
    "            ngram += words[i+j]\n",
    "            if j != n-1:\n",
    "                ngram += \" \"\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_ngrams_table_constructor(xs, ys):\n",
    "    table = {}\n",
    "    for x,y in zip(xs, ys):\n",
    "        for ngram in x:\n",
    "            if ngram not in table:\n",
    "                table[ngram] = [3, 1, 1]\n",
    "                if y == 1:\n",
    "                    table[ngram][1] += 1\n",
    "                else:\n",
    "                    table[ngram][2] += 1\n",
    "            else:\n",
    "                table[ngram][0] += 1\n",
    "                if y == 1:\n",
    "                    table[ngram][1] += 1\n",
    "                else:\n",
    "                    table[ngram][2] += 1\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_ngrams = {\n",
    "    \"train\":{\n",
    "        \"y\": train_y\n",
    "    },\n",
    "    \"test\":{\n",
    "        \"y\": test_y\n",
    "    }\n",
    "}\n",
    "\n",
    "for n in ns:\n",
    "    inputs_ngrams[\"train\"][n] = []\n",
    "    inputs_ngrams[\"test\"][n] = []\n",
    "    for entry in train_x:\n",
    "        inputs_ngrams[\"train\"][n].append(form_ngrams(entry.split(\" \"), n))\n",
    "    for entry in test_x:\n",
    "        inputs_ngrams[\"test\"][n].append(form_ngrams(entry.split(\" \"), n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {}\n",
    "for n in ns:\n",
    "    tables[n] = likelihood_ngrams_table_constructor(\n",
    "        inputs_ngrams[\"train\"][n],\n",
    "        inputs_ngrams[\"train\"][\"y\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proba_ngram(list_ngrams, table):\n",
    "    \"\"\"Please make sure that the table have been generated with the same n.\"\"\"\n",
    "    proba_pos = 1\n",
    "    proba_neg = 1\n",
    "    for ngram in list_ngrams:\n",
    "        if ngram in table:\n",
    "            proba_pos *= table[ngram][1]/table[ngram][0]\n",
    "            proba_neg *= table[ngram][2]/table[ngram][0]\n",
    "        else:\n",
    "            proba_pos *= 0.5\n",
    "            proba_neg *= 0.5\n",
    "    return proba_pos, proba_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_probas = {\n",
    "    \"train\":[],\n",
    "    \"test\":[],\n",
    "    \"y\":{\n",
    "        \"train\":[],\n",
    "        \"test\":[]\n",
    "    }\n",
    "}\n",
    "for n in ns:\n",
    "    for i,entry in enumerate(inputs_ngrams[\"train\"][n]):\n",
    "        if n == ns[0]:\n",
    "            inputs_probas[\"train\"].append([])\n",
    "        temp = get_proba_ngram(inputs_ngrams[\"train\"][n][i], tables[n])\n",
    "        inputs_probas[\"train\"][i].append(temp[0])\n",
    "        inputs_probas[\"train\"][i].append(temp[1])\n",
    "    for i,entry in enumerate(inputs_ngrams[\"test\"][n]):\n",
    "        if n == ns[0]:\n",
    "            inputs_probas[\"test\"].append([])\n",
    "        temp = get_proba_ngram(inputs_ngrams[\"test\"][n][i], tables[n])\n",
    "        inputs_probas[\"test\"][i].append(temp[0])\n",
    "        inputs_probas[\"test\"][i].append(temp[1])\n",
    "for i,y in enumerate(inputs_ngrams[\"train\"][\"y\"]):\n",
    "    inputs_probas[\"y\"][\"train\"].append(y)\n",
    "for i,y in enumerate(inputs_ngrams[\"test\"][\"y\"]):\n",
    "    inputs_probas[\"y\"][\"test\"].append(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Form the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(data=inputs_probas[\"train\"])\n",
    "y_train = pd.Series(data=inputs_probas[\"y\"][\"train\"])\n",
    "X_test = pd.DataFrame(data=inputs_probas[\"test\"])\n",
    "y_test = pd.Series(data=inputs_probas[\"y\"][\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "\n",
    "train_data = TrainData(\n",
    "    torch.FloatTensor(X_train), \n",
    "    torch.FloatTensor(y_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "\n",
    "test_data = TestData(torch.FloatTensor(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_data, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassification, self).__init__()        # Number of input features is 6.\n",
    "        self.layer_1 = nn.Linear(6, 64) \n",
    "        self.layer_2 = nn.Linear(64, 64)\n",
    "        self.layer_out = nn.Linear(64, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryClassification(\n",
      "  (layer_1): Linear(in_features=6, out_features=64, bias=True)\n",
      "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BinaryClassification()\n",
    "model.to(device)\n",
    "print(model)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.24543 | Acc: 87.742\n",
      "Epoch 002: | Loss: 0.21866 | Acc: 89.431\n",
      "Epoch 003: | Loss: 0.18534 | Acc: 91.350\n",
      "Epoch 004: | Loss: 0.17793 | Acc: 91.617\n",
      "Epoch 005: | Loss: 0.17353 | Acc: 91.839\n",
      "Epoch 006: | Loss: 0.17629 | Acc: 91.965\n",
      "Epoch 007: | Loss: 0.16438 | Acc: 92.395\n",
      "Epoch 008: | Loss: 0.16744 | Acc: 92.173\n",
      "Epoch 009: | Loss: 0.17042 | Acc: 92.282\n",
      "Epoch 010: | Loss: 0.17702 | Acc: 91.730\n",
      "Epoch 011: | Loss: 0.16841 | Acc: 92.336\n",
      "Epoch 012: | Loss: 0.16428 | Acc: 92.478\n",
      "Epoch 013: | Loss: 0.16768 | Acc: 92.082\n",
      "Epoch 014: | Loss: 0.15516 | Acc: 92.933\n",
      "Epoch 015: | Loss: 0.16196 | Acc: 92.623\n",
      "Epoch 016: | Loss: 0.24140 | Acc: 88.040\n",
      "Epoch 017: | Loss: 0.22150 | Acc: 89.478\n",
      "Epoch 018: | Loss: 0.20086 | Acc: 90.703\n",
      "Epoch 019: | Loss: 0.23972 | Acc: 88.256\n",
      "Epoch 020: | Loss: 0.24872 | Acc: 87.894\n",
      "Epoch 021: | Loss: 0.24788 | Acc: 87.845\n",
      "Epoch 022: | Loss: 0.23396 | Acc: 88.598\n",
      "Epoch 023: | Loss: 0.24866 | Acc: 87.681\n",
      "Epoch 024: | Loss: 0.25143 | Acc: 87.574\n",
      "Epoch 025: | Loss: 0.25485 | Acc: 87.469\n",
      "Epoch 026: | Loss: 0.25224 | Acc: 87.534\n",
      "Epoch 027: | Loss: 0.25853 | Acc: 87.119\n",
      "Epoch 028: | Loss: 0.24786 | Acc: 87.678\n",
      "Epoch 029: | Loss: 0.23978 | Acc: 88.166\n",
      "Epoch 030: | Loss: 0.24028 | Acc: 88.146\n",
      "Epoch 031: | Loss: 0.23318 | Acc: 88.446\n",
      "Epoch 032: | Loss: 0.22565 | Acc: 88.914\n",
      "Epoch 033: | Loss: 0.22454 | Acc: 89.160\n",
      "Epoch 034: | Loss: 0.22165 | Acc: 89.253\n",
      "Epoch 035: | Loss: 0.22309 | Acc: 89.192\n",
      "Epoch 036: | Loss: 0.24991 | Acc: 87.672\n",
      "Epoch 037: | Loss: 0.24121 | Acc: 88.061\n",
      "Epoch 038: | Loss: 0.25499 | Acc: 87.445\n",
      "Epoch 039: | Loss: 0.23601 | Acc: 88.404\n",
      "Epoch 040: | Loss: 0.22626 | Acc: 89.042\n",
      "Epoch 041: | Loss: 0.22865 | Acc: 88.790\n",
      "Epoch 042: | Loss: 0.23572 | Acc: 88.423\n",
      "Epoch 043: | Loss: 0.23940 | Acc: 88.168\n",
      "Epoch 044: | Loss: 0.22944 | Acc: 88.769\n",
      "Epoch 045: | Loss: 0.23091 | Acc: 88.621\n",
      "Epoch 046: | Loss: 0.22527 | Acc: 89.106\n",
      "Epoch 047: | Loss: 0.22208 | Acc: 89.161\n",
      "Epoch 048: | Loss: 0.22540 | Acc: 89.054\n",
      "Epoch 049: | Loss: 0.21735 | Acc: 89.507\n",
      "Epoch 050: | Loss: 0.21451 | Acc: 89.538\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [40000, 40002]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-154-046356929824>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m     \"\"\"\n\u001b[1;32m--> 296\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[0;32m    263\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [40000, 40002]"
     ]
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_list))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c19f1ca3afd4998816691da9a36584d647e992dcfc819c425cd38dca0566a7c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
