{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_POS = \"../text/twitter-datasets/train_pos.txt\"\n",
    "PATH_TO_NEG = \"../text/twitter-datasets/train_neg.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_train = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining the data from the files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General function to get data from a file for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_file(filename, proportion=None, isTraining=False, value=None, shuffle=False):\n",
    "    \"\"\"We assume here that (proportion != None) <=> local testing. \n",
    "    Please be sure to verify this before using the function.\n",
    "    The shuffle part is only used for the local testing phase, \n",
    "    where selecting different subsets to train/validate our model can be of influence\"\"\"\n",
    "    with open(filename, \"r\") as file:\n",
    "        content = file.read()\n",
    "        content_lines = content.split(\"\\n\")\n",
    "        if shuffle:\n",
    "            random.shuffle(content_lines)\n",
    "        if proportion != None:\n",
    "            # Here is the processing of training data during the local testing phase\n",
    "            temp1_x = content_lines[:int(len(content_lines) * proportion)]\n",
    "            temp1_y = [value] * int(len(content_lines) * proportion)\n",
    "            temp2_x = content_lines[int(len(content_lines) * proportion):]\n",
    "            temp2_y = [value] * int(len(content_lines) * (1-proportion))\n",
    "            return temp1_x, temp1_y, temp2_x, temp2_y\n",
    "        if isTraining:\n",
    "            # Here is the processing of training data during the real prediction phase\n",
    "            temp_x = content_lines[:]\n",
    "            temp_y = [value] * len(content_lines)\n",
    "            return temp_x, temp_y\n",
    "        # Here is the processing of new data for the real prediction phase\n",
    "        temp_ids = []\n",
    "        temp_xs = []\n",
    "        for i in range(len(content_lines)):\n",
    "            if \",\" in content_lines[i]:\n",
    "                entrySplitted = re.split(\",\", content_lines[i], 1)\n",
    "                temp_ids.append(entrySplitted[0])\n",
    "                temp_xs.append(entrySplitted[1])\n",
    "        return temp_ids, temp_xs       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual recuperation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "test_x = []\n",
    "test_y = []\n",
    "\n",
    "temp_train_x, temp_train_y, temp_test_x, temp_test_y = get_data_from_file(\n",
    "    PATH_TO_POS, \n",
    "    proportion=proportion_train,\n",
    "    isTraining=True,\n",
    "    value=1,\n",
    "    shuffle=False\n",
    ")\n",
    "train_x += temp_train_x\n",
    "train_y += temp_train_y\n",
    "test_x += temp_test_x\n",
    "test_y += temp_test_y\n",
    "temp_train_x, temp_train_y, temp_test_x, temp_test_y = get_data_from_file(\n",
    "    PATH_TO_NEG, \n",
    "    proportion=proportion_train,\n",
    "    isTraining=True,\n",
    "    value=-1,\n",
    "    shuffle=False\n",
    ")\n",
    "train_x += temp_train_x\n",
    "train_y += temp_train_y\n",
    "test_x += temp_test_x\n",
    "test_y += temp_test_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same thing as above, but without using a function. Can rely on this should the function introduce too much errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(PATH_TO_POS, \"r\") as file:\n",
    "#     content = file.read()\n",
    "#     content_lines = content.split(\"\\n\")\n",
    "#     train_x += content_lines[:int(len(content_lines) * proportion_train)]\n",
    "#     train_y += [1] * int(len(content_lines) * proportion_train)\n",
    "#     test_x += content_lines[int(len(content_lines) * proportion_train):]\n",
    "#     test_y += [1] * int(len(content_lines) * (1-proportion_train))\n",
    "\n",
    "# with open(PATH_TO_NEG, \"r\") as file:\n",
    "#     content = file.read()\n",
    "#     content_lines = content.split(\"\\n\")\n",
    "#     train_x += content_lines[:int(len(content_lines) * proportion_train)]\n",
    "#     train_y += [-1] * int(len(content_lines) * proportion_train) \n",
    "#     test_x += content_lines[int(len(content_lines) * proportion_train):]\n",
    "#     test_y += [-1] * int(len(content_lines) * (1-proportion_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing the tables with likelihood to be a positive or negative element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First form the bigrams for each sentence (the sentence already split in a list of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_ngrams(words, n):\n",
    "    ngrams = []\n",
    "    number_ngrams = len(words)-n+1\n",
    "    for i in range(number_ngrams):\n",
    "        ngram = \"\"\n",
    "        for j in range(n):\n",
    "            ngram += words[i+j]\n",
    "            if j != n-1:\n",
    "                ngram += \" \"\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_table_constructor(table, xs, ys, n=2):\n",
    "    table.clear()\n",
    "    for x,y in zip(xs, ys):\n",
    "        list_words = x.split(\" \")\n",
    "        list_ngrams = form_ngrams(list_words, n=n)\n",
    "        for ngram in list_ngrams:\n",
    "            if ngram not in table:\n",
    "                table[ngram] = [3, 1, 1]\n",
    "                if y == 1:\n",
    "                    table[ngram][1] += 1\n",
    "                else:\n",
    "                    table[ngram][2] += 1\n",
    "            else:\n",
    "                table[ngram][0] += 1\n",
    "                if y == 1:\n",
    "                    table[ngram][1] += 1\n",
    "                else:\n",
    "                    table[ngram][2] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the previous function to build the table, with the correct parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {}\n",
    "bigrams = {}\n",
    "trigrams = {}\n",
    "\n",
    "likelihood_table_constructor(\n",
    "    words,\n",
    "    train_x,\n",
    "    train_y,\n",
    "    n=1\n",
    ")\n",
    "likelihood_table_constructor(\n",
    "    bigrams,\n",
    "    train_x,\n",
    "    train_y,\n",
    "    n=2\n",
    ")\n",
    "likelihood_table_constructor(\n",
    "    trigrams,\n",
    "    train_x,\n",
    "    train_y,\n",
    "    n=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Less generalized way to do than the above. If this is used, must use the archive way of classifiers too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate a words dictionary, without using the function. Can rely on this should the function introduce too much errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = {}\n",
    "# for x,y in zip(train_x, train_y):\n",
    "#     list_words = x.split(\" \")\n",
    "#     for w in list_words:\n",
    "#         if w not in words:\n",
    "#             words[w] = [3, 1, 1]\n",
    "#             if y == 1:\n",
    "#                 words[w][1] += 1\n",
    "#             else:\n",
    "#                 words[w][2] += 1\n",
    "#         else:\n",
    "#             words[w][0] += 1\n",
    "#             if y == 1:\n",
    "#                 words[w][1] += 1\n",
    "#             else:\n",
    "#                 words[w][2] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same as above, but for bigrams. Should also be covered by the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigrams = {}\n",
    "# for x,y in zip(train_x, train_y):\n",
    "#     list_words = x.split(\" \")\n",
    "#     list_bigrams = form_ngrams(list_words, n=2)\n",
    "#     for b in list_bigrams:\n",
    "#         if b not in bigrams:\n",
    "#             bigrams[b] = [3, 1, 1]\n",
    "#             if y == 1:\n",
    "#                 bigrams[b][1] += 1\n",
    "#             else:\n",
    "#                 bigrams[b][2] += 1\n",
    "#         else:\n",
    "#             bigrams[b][0] += 1\n",
    "#             if y == 1:\n",
    "#                 bigrams[b][1] += 1\n",
    "#             else:\n",
    "#                 bigrams[b][2] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic classifiers for a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier based on one type of n-gram only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_ngrams(sentence, table, n=2):\n",
    "    \"\"\"table parameter MUST have been generated using the same n.\n",
    "    Please be sure to check for this before using the function.\"\"\"\n",
    "    proba_pos = 1\n",
    "    proba_neg = 1\n",
    "    list_words = sentence.split(\" \")\n",
    "    list_ngrams = form_ngrams(list_words, n=n)\n",
    "    for ngram in list_ngrams:\n",
    "        if ngram in table:\n",
    "            proba_pos *= table[ngram][1]/table[ngram][0]\n",
    "            proba_neg *= table[ngram][2]/table[ngram][0]\n",
    "        else:\n",
    "            proba_pos *= 0.5\n",
    "            proba_neg *= 0.5\n",
    "    if proba_pos >= proba_neg:\n",
    "        return 1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same thing as for the other archives, can rely on this should the functions crash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Classifier based on words only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def classifier_words(sentence):\n",
    "#     proba_pos = 1\n",
    "#     proba_neg = 1\n",
    "#     list_words = sentence.split(\" \")\n",
    "#     for w in list_words:\n",
    "#         if w in words:\n",
    "#             proba_pos *= words[w][1]/words[w][0]\n",
    "#             proba_neg *= words[w][2]/words[w][0]\n",
    "#         else:\n",
    "#             proba_pos *= 0.5\n",
    "#             proba_neg *= 0.5\n",
    "#     if proba_pos >= proba_neg:\n",
    "#         return 1\n",
    "#     return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Classifier based on bigrams only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def classifier_bigrams(sentence):\n",
    "#     proba_pos = 1\n",
    "#     proba_neg = 1\n",
    "#     list_words = sentence.split(\" \")\n",
    "#     list_bigrams = form_ngrams(list_words, n=2)\n",
    "#     for b in list_bigrams:\n",
    "#         if b in bigrams:\n",
    "#             proba_pos *= bigrams[b][1]/bigrams[b][0]\n",
    "#             proba_neg *= bigrams[b][2]/bigrams[b][0]\n",
    "#         else:\n",
    "#             proba_pos *= 0.5\n",
    "#             proba_neg *= 0.5\n",
    "#     if proba_pos >= proba_neg:\n",
    "#         return 1\n",
    "#     return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(xs, ys, classifier, table, n):\n",
    "    matrix = [[0, 0], [0, 0]]\n",
    "    for x,y in zip(xs, ys):\n",
    "        if classifier(x, table, n=n) == y:\n",
    "            if y == 1:\n",
    "                matrix[0][0] += 1\n",
    "            else:\n",
    "                matrix[1][1] += 1\n",
    "        else:\n",
    "            if y == 1:\n",
    "                matrix[1][0] += 1\n",
    "            else:\n",
    "                matrix[0][1] += 1\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14932, 5346]\n",
      "[5068, 14654]\n",
      "\n",
      "[17293, 5319]\n",
      "[2707, 14681]\n",
      "\n",
      "[16605, 5843]\n",
      "[3395, 14157]\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_words = get_confusion_matrix(test_x, test_y, classifier_ngrams, words, 1)\n",
    "confusion_matrix_bigrams = get_confusion_matrix(test_x, test_y, classifier_ngrams, bigrams, 2)\n",
    "confusion_matrix_trigrams = get_confusion_matrix(test_x, test_y, classifier_ngrams, trigrams, 3)\n",
    "\n",
    "print(confusion_matrix_words[0])\n",
    "print(confusion_matrix_words[1])\n",
    "print(\"\")\n",
    "print(confusion_matrix_bigrams[0])\n",
    "print(confusion_matrix_bigrams[1])\n",
    "print(\"\")\n",
    "print(confusion_matrix_trigrams[0])\n",
    "print(confusion_matrix_trigrams[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. F-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision(matrix):\n",
    "    true_pos = matrix[0][0]\n",
    "    false_pos = matrix[0][1]\n",
    "    return true_pos / (true_pos+false_pos)\n",
    "def compute_recall(matrix):\n",
    "    true_pos = matrix[0][0]\n",
    "    false_neg = matrix[1][0]\n",
    "    return true_pos / (true_pos+false_neg)\n",
    "def compute_fscore(matrix):\n",
    "    p = compute_precision(matrix)\n",
    "    r = compute_recall(matrix)\n",
    "    return 2 * (p*r) / (p+r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words only: precision=0.736, recall=0.747, f-score=0.741\n",
      "Bigrams only: precision=0.765, recall=0.865, f-score=0.812\n",
      "Trigrams only: precision=0.740, recall=0.830, f-score=0.782\n"
     ]
    }
   ],
   "source": [
    "print(\"Words only: precision={:.3f}, recall={:.3f}, f-score={:.3f}\".format(\n",
    "    compute_precision(confusion_matrix_words),\n",
    "    compute_recall(confusion_matrix_words),\n",
    "    compute_fscore(confusion_matrix_words)\n",
    "))\n",
    "print(\"Bigrams only: precision={:.3f}, recall={:.3f}, f-score={:.3f}\".format(\n",
    "    compute_precision(confusion_matrix_bigrams),\n",
    "    compute_recall(confusion_matrix_bigrams),\n",
    "    compute_fscore(confusion_matrix_bigrams)\n",
    "))\n",
    "print(\"Trigrams only: precision={:.3f}, recall={:.3f}, f-score={:.3f}\".format(\n",
    "    compute_precision(confusion_matrix_trigrams),\n",
    "    compute_recall(confusion_matrix_trigrams),\n",
    "    compute_fscore(confusion_matrix_trigrams)\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
