{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_POS = \"../text/twitter-datasets/train_pos_full.txt\"\n",
    "PATH_TO_NEG = \"../text/twitter-datasets/train_neg_full.txt\"\n",
    "PATH_TO_TEST = \"../text/twitter-datasets/test_data.txt\"\n",
    "PATH_TO_SUB = \"./submission.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining the data from the files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General function to get data from a file for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_file(filename, proportion=None, isTraining=False, value=None, shuffle=False):\n",
    "    \"\"\"We assume here that (proportion != None) <=> local testing. \n",
    "    Please be sure to verify this before using the function.\n",
    "    The shuffle part is only used for the local testing phase, \n",
    "    where selecting different subsets to train/validate our model can be of influence\"\"\"\n",
    "    with open(filename, \"r\") as file:\n",
    "        content = file.read()\n",
    "        content_lines = content.split(\"\\n\")\n",
    "        if shuffle:\n",
    "            random.shuffle(content_lines)\n",
    "        if proportion != None:\n",
    "            # Here is the processing of training data during the local testing phase\n",
    "            temp1_x = content_lines[:int(len(content_lines) * proportion)]\n",
    "            temp1_y = [value] * int(len(content_lines) * proportion)\n",
    "            temp2_x = content_lines[int(len(content_lines) * proportion):]\n",
    "            temp2_y = [value] * int(len(content_lines) * (1-proportion))\n",
    "            return temp1_x, temp1_y, temp2_x, temp2_y\n",
    "        if isTraining:\n",
    "            # Here is the processing of training data during the real prediction phase\n",
    "            temp_x = content_lines[:]\n",
    "            temp_y = [value] * len(content_lines)\n",
    "            return temp_x, temp_y\n",
    "        # Here is the processing of new data for the real prediction phase\n",
    "        temp_ids = []\n",
    "        temp_xs = []\n",
    "        for i in range(len(content_lines)):\n",
    "            if \",\" in content_lines[i]:\n",
    "                entrySplitted = re.split(\",\", content_lines[i], 1)\n",
    "                temp_ids.append(entrySplitted[0])\n",
    "                temp_xs.append(entrySplitted[1])\n",
    "        return temp_ids, temp_xs        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual recuperation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "temp_train_x, temp_train_y = get_data_from_file(\n",
    "    PATH_TO_POS,\n",
    "    isTraining=True,\n",
    "    value=1,\n",
    "    shuffle=False\n",
    ")\n",
    "train_x += temp_train_x\n",
    "train_y += temp_train_y\n",
    "temp_train_x, temp_train_y = get_data_from_file(\n",
    "    PATH_TO_NEG,\n",
    "    isTraining=True,\n",
    "    value=-1,\n",
    "    shuffle=False\n",
    ")\n",
    "train_x += temp_train_x\n",
    "train_y += temp_train_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing the tables with likelihood to be a positive or negative element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First form the bigrams for each sentence (the sentence already split in a list of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_ngrams(words, n):\n",
    "    ngrams = []\n",
    "    number_ngrams = len(words)-n+1\n",
    "    for i in range(number_ngrams):\n",
    "        ngram = \"\"\n",
    "        for j in range(n):\n",
    "            ngram += words[i+j]\n",
    "            if j != n-1:\n",
    "                ngram += \" \"\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_table_constructor(xs, ys, n=2):\n",
    "    table = {}\n",
    "    for x,y in zip(xs, ys):\n",
    "        list_words = x.split(\" \")\n",
    "        list_ngrams = form_ngrams(list_words, n=n)\n",
    "        for ngram in list_ngrams:\n",
    "            if ngram not in table:\n",
    "                table[ngram] = [3, 1, 1]\n",
    "                if y == 1:\n",
    "                    table[ngram][1] += 1\n",
    "                else:\n",
    "                    table[ngram][2] += 1\n",
    "            else:\n",
    "                table[ngram][0] += 1\n",
    "                if y == 1:\n",
    "                    table[ngram][1] += 1\n",
    "                else:\n",
    "                    table[ngram][2] += 1\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the previous function to build the table, with the correct parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = [1, 2, 3]\n",
    "tables = {}\n",
    "for n in ns:\n",
    "    tables[n] = likelihood_table_constructor(\n",
    "        train_x,\n",
    "        train_y,\n",
    "        n=n\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic classifiers for a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier based on multiple ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_ngrams_mean(sentence, tables, ns, coeffs):\n",
    "    proba_poss = []\n",
    "    proba_negs = []\n",
    "    list_words = sentence.split(\" \")\n",
    "    for n in ns:\n",
    "        proba_pos = 1\n",
    "        proba_neg = 1\n",
    "        list_ngrams = form_ngrams(list_words, n=n)\n",
    "        for ngram in list_ngrams:\n",
    "            if ngram in tables[n]:\n",
    "                proba_pos *= tables[n][ngram][1]/tables[n][ngram][0]\n",
    "                proba_neg *= tables[n][ngram][2]/tables[n][ngram][0]\n",
    "            else:\n",
    "                proba_pos *= 0.5\n",
    "                proba_neg *= 0.5\n",
    "        proba_poss.append(proba_pos)\n",
    "        proba_negs.append(proba_neg)\n",
    "    proba_pos = 0\n",
    "    proba_neg = 0\n",
    "    for ppos,pneg,coeff in zip(proba_poss, proba_negs, coeffs):\n",
    "        proba_pos += coeff*ppos\n",
    "        proba_neg += coeff*pneg\n",
    "    if proba_pos >= proba_neg:\n",
    "        return 1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction and submission file creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids, test_xs = get_data_from_file(PATH_TO_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_write(tables, ns, xs=test_xs, classifier=classifier_ngrams_mean):\n",
    "    # Predictions\n",
    "    test_preds = []\n",
    "    for x in xs:\n",
    "        test_preds.append(classifier(x, tables, ns, [(1-0.802)/2, 0.802, (1-0.802)/2]))\n",
    "\n",
    "    # Writing\n",
    "    with open(PATH_TO_SUB, \"w\") as file:\n",
    "        file.write(\"Id,Prediction\\n\")\n",
    "        for id,pred in zip(test_ids, test_preds):\n",
    "            file.write(\"{},{}\\n\".format(id, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_write(tables, ns)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
